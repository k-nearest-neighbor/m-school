---
title: Benchmarks & Datasets
template: content
nextjs:
  metadata:
    title: Benchmarks & Datasets
    description: Benchmarks & Datasets for AI Agent and code generation research.
---

---

## Coding Ability

### HumanEval

[Website](https://github.com/openai/human-eval) | [HuggingFace](https://huggingface.co/datasets/openai_humaneval) | [Chen et al. 2021](https://arxiv.org/abs/2107.03374)

Introduced in 2021 by **OpenAI**, along with the "**Codex**" model.

Related AI Breakout posts:

- ðŸ““ What does Pass@1 mean?

---

### HumanEval+

[Website](https://github.com/openai/human-eval) | [HuggingFace](https://huggingface.co/datasets/openai_humaneval) | [Chen et al. 2021](https://arxiv.org/abs/2107.03374)

...

---

### MBPP

[Website](https://github.com/openai/human-eval) | [HuggingFace](https://huggingface.co/datasets/openai_humaneval) | [Chen et al. 2021](https://arxiv.org/abs/2107.03374)

...

---

### CodingAgentEvals

[Website](https://www.ai-maintainer.com/benchmarking) | [Leaderboard](https://www.ai-maintainer.com/leaderboard)

Introduced in 2023 by **AI Maintainer** CodingAgentEvals is a benchmarking system addressing practical / industry use-cases for code generation and AI coding agents.

(I am a co-founder of AI Maintainer).
