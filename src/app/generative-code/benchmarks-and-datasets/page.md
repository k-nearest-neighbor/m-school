---
title: Benchmarks & Datasets
template: content
nextjs:
  metadata:
    title: Benchmarks & Datasets
    description: Benchmarks & Datasets for AI Agent and code generation research.
---

---

## Coding Ability

### HumanEval

[Website](https://github.com/openai/human-eval) | [HuggingFace](https://huggingface.co/datasets/openai_humaneval) | [Chen et al. 2021](https://arxiv.org/abs/2107.03374)

Introduced in 2021 by **OpenAI**, along with the "**Codex**" model.

<!-- Related AI Breakout posts:

- ðŸ““ What does Pass@1 mean? -->

---

### EvalPlus / HumanEval+

[Website](https://github.com/evalplus/evalplus) | [Liu et. al. 2023](https://arxiv.org/abs/2305.01210)

- Adds 81x more tests for HumanEval
- Tools for working with the test inputs and outputs

---

### MBPP

[Website](https://github.com/google-research/google-research/tree/master/mbpp) | [HuggingFace](https://huggingface.co/datasets/mbpp) | [Austin et al. 2021](https://arxiv.org/abs/2108.07732)

The **Mostly Basic Python Problems** dataset,

> "consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."

---

### MultiPL-E

[Website](https://nuprl.github.io/MultiPL-E/) | [HuggingFace](https://huggingface.co/datasets/nuprl/MultiPL-E) | [Cassano et al. 2022](https://arxiv.org/abs/2208.08227)

Extends HumanEval and MBPP into 18 additional languages.

<!-- ---

### CodingAgentEvals

[Website](https://www.ai-maintainer.com/benchmarking) | [Leaderboard](https://www.ai-maintainer.com/leaderboard)

Introduced in 2023 by **AI Maintainer** CodingAgentEvals is a benchmarking system addressing practical / industry use-cases for code generation and AI coding agents.

(I am a co-founder of AI Maintainer). -->
